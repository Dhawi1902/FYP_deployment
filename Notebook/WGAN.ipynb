{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from tensorflow.keras import layers, Sequential, datasets, Model\n",
    "import keras.src.saving\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/home/dhawi/Documents/dataset\"\n",
    "dataset = data_folder + \"/AI_project\"\n",
    "model_folder = \"/home/dhawi/Documents/model\"\n",
    "history_folder = \"/home/dhawi/Documents/History\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH  = 128\n",
    "IMG_HEIGHT = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "def load_images_from_folder(folder, subfolder):\n",
    "    images = []\n",
    "    gray = []\n",
    "    # lab = []\n",
    "    foldername = os.path.join(folder, subfolder)\n",
    "    for sub in os.listdir(foldername):\n",
    "        subfoldername = os.path.join(foldername, sub)\n",
    "        for filename in tqdm(os.listdir(subfoldername)):\n",
    "            img = cv2.imread(os.path.join(subfoldername, filename))\n",
    "            img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "        # convert the image to RGB (images are read in BGR in OpenCV)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            gry = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            if img is not None:\n",
    "                images.append(img/255.0)\n",
    "                gray.append(gry)\n",
    "                # lab.append(label)\n",
    "    return np.array(images), np.array(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caries, caries_gray = load_images_from_folder(dataset, \"Caries\")\n",
    "gingivitis, gingivitis_gray = load_images_from_folder(dataset, \"Gingivitis\")\n",
    "wsl, wsl_gray = load_images_from_folder(dataset, \"White Spot Lesion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = tf.random.normal((60, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import ops\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images):\n",
    "    for i in range(20):\n",
    "        plt.subplot(4, 5, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "# show_images(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    Discriminator = Sequential([\n",
    "        # First convolutional layer\n",
    "        layers.Conv2D(256, kernel_size=(3, 3), strides=2, padding='same', input_shape=(128, 128, 3)),\n",
    "        layers.LeakyReLU(alpha=0.2),  # Leaky ReLU activation\n",
    "\n",
    "        # Second convolutional layer\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),  # Batch normalization for stability\n",
    "\n",
    "        # Third convolutional layer\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        # Flatten and final output layer\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)  # Output single value (validity score)\n",
    "    ])\n",
    "    return Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    Generator = Sequential([\n",
    "        # First dense layer\n",
    "        layers.Dense(8 * 8 * 128, input_shape=(1024,)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        # Reshape the dense output into (8, 8, 128)\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "\n",
    "        # First Conv2DTranspose layer: Upsamples to (16, 16, 128)\n",
    "        layers.Conv2DTranspose(128, kernel_size=(3, 3), strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        # Second Conv2DTranspose layer: Upsamples to (32, 32, 64)\n",
    "        layers.Conv2DTranspose(64, kernel_size=(3, 3), strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        # Third Conv2DTranspose layer: Upsamples to (64, 64, 32)\n",
    "        layers.Conv2DTranspose(32, kernel_size=(3, 3), strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        # Fourth Conv2DTranspose layer: Upsamples to (128, 128, 3) with sigmoid activation for image output\n",
    "        layers.Conv2DTranspose(3, kernel_size=(3, 3), strides=2, padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    return Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim, lambda_gp=10):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.history = {\"Generator Loss\": [], \"Discriminator Loss\": []}\n",
    "\n",
    "    def compile(self, gen_optimizer, disc_optimizer, criterion=None):\n",
    "        super(WGAN, self).compile()\n",
    "        self.generator_optimizer = gen_optimizer\n",
    "        self.discriminator_optimizer = disc_optimizer\n",
    "        self.cross_entropy = criterion\n",
    "\n",
    "    def gradient_penalty(self, real_images, fake_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        interpolated_images = real_images * alpha + fake_images * (1 - alpha)\n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_images)\n",
    "            pred = self.discriminator(interpolated_images, training=True)\n",
    "        \n",
    "        grads = gp_tape.gradient(pred, interpolated_images)\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output, gp):\n",
    "        real_loss = -tf.reduce_mean(real_output)\n",
    "        fake_loss = tf.reduce_mean(fake_output)\n",
    "        total_loss = real_loss + fake_loss + self.lambda_gp * gp\n",
    "        return total_loss\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        return -tf.reduce_mean(fake_output)\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        # Sample random noise\n",
    "        noise = tf.random.normal((batch_size, self.latent_dim))\n",
    "        \n",
    "        # Train Discriminator\n",
    "        for _ in range(5):  # WGAN usually trains the discriminator more than the generator\n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                fake_images = self.generator(noise, training=True)\n",
    "                \n",
    "                real_output = self.discriminator(real_images, training=True)\n",
    "                fake_output = self.discriminator(fake_images, training=True)\n",
    "\n",
    "                # Gradient Penalty\n",
    "                gp = self.gradient_penalty(real_images, fake_images)\n",
    "                \n",
    "                # Calculate the loss for the discriminator\n",
    "                disc_loss = self.discriminator_loss(real_output, fake_output, gp)\n",
    "            \n",
    "            disc_grads = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "            self.discriminator_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train Generator\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            generated_images = self.generator(noise, training=True)\n",
    "            fake_output = self.discriminator(generated_images, training=True)\n",
    "            gen_loss = self.generator_loss(fake_output)\n",
    "\n",
    "        gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "\n",
    "        # Return loss metrics for Keras's `fit` to log\n",
    "        return {\"Generator Loss\": gen_loss, \"Discriminator Loss\": disc_loss}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name):\n",
    "    # clear the session for a clean run\n",
    "    keras.backend.clear_session()\n",
    "    vae_encoder = model_folder + \"/\" + model_name + \"_encoder.h5\"\n",
    "    vae_decoder = model_folder + \"/\" + model_name + \"_decoder.h5\"\n",
    "    encoder = keras.src.saving.load_model(vae_encoder, custom_objects={'Sampling': Sampling}, compile=False)\n",
    "    decoder = keras.src.saving.load_model(vae_decoder, compile=False)\n",
    "    Generator = build_generator()\n",
    "    Discriminator = build_discriminator()\n",
    "    edgan_model = EDGAN(Generator, Discriminator, encoder, decoder)\n",
    "    edgan_model.compile(gen_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "                  disc_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "                  criterion = tf.keras.losses.BinaryCrossentropy(True))\n",
    "    \n",
    "    return edgan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def show_history(history, model_name):\n",
    "    plt.plot(history.history['Generator Loss'])\n",
    "    plt.plot(history.history['Discriminator Loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    # plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Get the dictionary containing each metric and the loss for each epoch\n",
    "    history_dict = history.history\n",
    "    # Save it under the form of a json file\n",
    "    history_file = history_folder + \"/\" + model_name + \"_history.json\"\n",
    "    json.dump(history_dict, open(history_file, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    generator_path = model_folder + \"/\" + model_name + \"_encoder.h5\"\n",
    "    discriminator_path = model_folder + \"/\" + model_name + \"_decoder.h5\"\n",
    "    model_path = model_folder + \"/\" + model_name + \"_model.h5\"\n",
    "    model.generator.save(generator_path)\n",
    "    model.discriminator.save(discriminator_path)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = tf.keras.optimizers.Adam(learning_rate=opt.lr, beta_1=opt.b1, beta_2=opt.b2)\n",
    "optimizer_D = tf.keras.optimizers.Adam(learning_rate=opt.lr, beta_1=opt.b1, beta_2=opt.b2)\n",
    "\n",
    "# Loss functions\n",
    "def d_loss_fn(real_validity, fake_validity, gradient_penalty_loss):\n",
    "    return -tf.reduce_mean(real_validity) + tf.reduce_mean(fake_validity) + lambda_gp * gradient_penalty_loss\n",
    "\n",
    "def g_loss_fn(fake_validity):\n",
    "    return -tf.reduce_mean(fake_validity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
